{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "006afb73-ce8f-40ad-be54-f9a2089cbe29",
   "metadata": {},
   "source": [
    "# 1) Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "816ecdcb-289c-454c-8113-6cf11ead5d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm\", 'learning', 'AI/ML', '—', \"it's\", 'fun!', '2025', 'is', 'great.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def word_tokenization(text):\n",
    "    # tokens = re.findall(r\"\\w+['-]?\\w*|\\d+|[^\\s\\w]\", text) #using regx\n",
    "    tokens = text.split() #using split function\n",
    "    return tokens\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"I'm learning AI/ML — it's fun! 2025 is great.\"\n",
    "    print(word_tokenization(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782eca8a-5758-407a-9bd6-3806201a2f8a",
   "metadata": {},
   "source": [
    "# 2) Character tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c343a8e1-16c5-4c24-92a4-4a0dd678fe16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h', 'e', 'l', 'l', 'o', 'w', 'o', 'r', 'l', 'd']\n",
      "['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']\n"
     ]
    }
   ],
   "source": [
    "def character_tokenization(text, include_spaces = False):\n",
    "    l = []\n",
    "    if include_spaces:\n",
    "        return list(text)\n",
    "    else:\n",
    "        for char in text:\n",
    "            if not char.isspace():\n",
    "                l.append(char)\n",
    "    return l\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"hello world\"\n",
    "    print(character_tokenization(text))\n",
    "    print(character_tokenization(text, True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919d1076-b400-495d-8e0e-6dcd2894c41a",
   "metadata": {},
   "source": [
    "# 3) Subword tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d5d9bcc-f8d2-4e19-a36f-0848d73b5ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['playground']\n",
      "['play', 'ing']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def build_vocab_from_words(words, min_count=1):\n",
    "    c = Counter(' '.join(words).ssplit())\n",
    "    return set(w for w, cnt in c.items() if cnt >= min_count)\n",
    "\n",
    "def greedy_subword_tokenize(word, vocab, unk_token = '[UNK]'):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        match = None\n",
    "        for j in range(len(word), i, -1):\n",
    "            piece = word[i:j]\n",
    "            if piece in vocab:\n",
    "                match = piece\n",
    "                tokens.append(piece)\n",
    "                i = j\n",
    "                break\n",
    "        if match is None:\n",
    "            return [unk_token]\n",
    "    return tokens\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vocab = {'play', 'ing', 'play', 'ground', 'able', 'playground'}\n",
    "    print(greedy_subword_tokenize('playground', vocab)) \n",
    "    print(greedy_subword_tokenize('playing', vocab)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939558aa-4048-4770-bb30-6e8ead2be286",
   "metadata": {},
   "source": [
    "# 4) Byte-Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c8f0a7-9028-47a7-843c-190edb762eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple_bpe.py\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[(symbols[i], symbols[i+1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    pattern = re.compile(r'(?<!\\S)'+re.escape(bigram)+r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        new_word = pattern.sub(replacement, word)\n",
    "        v_out[new_word] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "def train_bpe(corpus, num_merges=50):\n",
    "    # corpus: list of words (strings)\n",
    "    vocab = Counter()\n",
    "    for w in corpus:\n",
    "        vocab[' '.join(list(w)) + ' </w>'] += 1\n",
    "    merges = []\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "        merges.append(best)\n",
    "    return merges, vocab\n",
    "\n",
    "def apply_bpe(token, merges):\n",
    "    word = ' '.join(list(token)) + ' </w>'\n",
    "    for a,b in merges:\n",
    "        bigram = a + ' ' + b\n",
    "        word = word.replace(bigram, a+b)\n",
    "    return word.split()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    corpus = [\"low\", \"lower\", \"newest\", \"widest\", \"low\", \"low\", \"lower\"]\n",
    "    merges, final_vocab = train_bpe(corpus, num_merges=10)\n",
    "    print(\"merges:\", merges)\n",
    "    print(\"tokenized:\", apply_bpe(\"lower\", merges))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
