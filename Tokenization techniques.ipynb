{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "006afb73-ce8f-40ad-be54-f9a2089cbe29",
   "metadata": {},
   "source": [
    "# 1) Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "816ecdcb-289c-454c-8113-6cf11ead5d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm\", 'learning', 'AI/ML', '—', \"it's\", 'fun!', '2025', 'is', 'great.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def word_tokenization(text):\n",
    "    # tokens = re.findall(r\"\\w+['-]?\\w*|\\d+|[^\\s\\w]\", text) #using regx\n",
    "    tokens = text.split() #using split function\n",
    "    return tokens\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"I'm learning AI/ML — it's fun! 2025 is great.\"\n",
    "    print(word_tokenization(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782eca8a-5758-407a-9bd6-3806201a2f8a",
   "metadata": {},
   "source": [
    "# 2) Character tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c343a8e1-16c5-4c24-92a4-4a0dd678fe16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h', 'e', 'l', 'l', 'o', 'w', 'o', 'r', 'l', 'd']\n",
      "['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']\n"
     ]
    }
   ],
   "source": [
    "def character_tokenization(text, include_spaces = False):\n",
    "    l = []\n",
    "    if include_spaces:\n",
    "        return list(text)\n",
    "    else:\n",
    "        for char in text:\n",
    "            if not char.isspace():\n",
    "                l.append(char)\n",
    "    return l\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"hello world\"\n",
    "    print(character_tokenization(text))\n",
    "    print(character_tokenization(text, True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919d1076-b400-495d-8e0e-6dcd2894c41a",
   "metadata": {},
   "source": [
    "# 3) Subword tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d5d9bcc-f8d2-4e19-a36f-0848d73b5ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['playground']\n",
      "['play', 'ing']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def build_vocab_from_words(words, min_count=1):\n",
    "    c = Counter(' '.join(words).ssplit())\n",
    "    return set(w for w, cnt in c.items() if cnt >= min_count)\n",
    "\n",
    "def greedy_subword_tokenize(word, vocab, unk_token = '[UNK]'):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        match = None\n",
    "        for j in range(len(word), i, -1):\n",
    "            piece = word[i:j]\n",
    "            if piece in vocab:\n",
    "                match = piece\n",
    "                tokens.append(piece)\n",
    "                i = j\n",
    "                break\n",
    "        if match is None:\n",
    "            return [unk_token]\n",
    "    return tokens\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vocab = {'play', 'ing', 'play', 'ground', 'able', 'playground'}\n",
    "    print(greedy_subword_tokenize('playground', vocab)) \n",
    "    print(greedy_subword_tokenize('playing', vocab)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca650b22-25e9-49d9-8497-d03eaaf47ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
