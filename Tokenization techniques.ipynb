{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "006afb73-ce8f-40ad-be54-f9a2089cbe29",
   "metadata": {},
   "source": [
    "# 1) Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "816ecdcb-289c-454c-8113-6cf11ead5d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm\", 'learning', 'AI/ML', '—', \"it's\", 'fun!', '2025', 'is', 'great.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def word_tokenization(text):\n",
    "    # tokens = re.findall(r\"\\w+['-]?\\w*|\\d+|[^\\s\\w]\", text) #using regx\n",
    "    tokens = text.split() #using split function\n",
    "    return tokens\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"I'm learning AI/ML — it's fun! 2025 is great.\"\n",
    "    print(word_tokenization(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782eca8a-5758-407a-9bd6-3806201a2f8a",
   "metadata": {},
   "source": [
    "# 2) Character tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c343a8e1-16c5-4c24-92a4-4a0dd678fe16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h', 'e', 'l', 'l', 'o', 'w', 'o', 'r', 'l', 'd']\n",
      "['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']\n"
     ]
    }
   ],
   "source": [
    "def character_tokenization(text, include_spaces = False):\n",
    "    l = []\n",
    "    if include_spaces:\n",
    "        return list(text)\n",
    "    else:\n",
    "        for char in text:\n",
    "            if not char.isspace():\n",
    "                l.append(char)\n",
    "    return l\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"hello world\"\n",
    "    print(character_tokenization(text))\n",
    "    print(character_tokenization(text, True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919d1076-b400-495d-8e0e-6dcd2894c41a",
   "metadata": {},
   "source": [
    "# 3) Subword tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d5d9bcc-f8d2-4e19-a36f-0848d73b5ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['playground']\n",
      "['play', 'ing']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def build_vocab_from_words(words, min_count=1):\n",
    "    c = Counter(' '.join(words).ssplit())\n",
    "    return set(w for w, cnt in c.items() if cnt >= min_count)\n",
    "\n",
    "def greedy_subword_tokenize(word, vocab, unk_token = '[UNK]'):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        match = None\n",
    "        for j in range(len(word), i, -1):\n",
    "            piece = word[i:j]\n",
    "            if piece in vocab:\n",
    "                match = piece\n",
    "                tokens.append(piece)\n",
    "                i = j\n",
    "                break\n",
    "        if match is None:\n",
    "            return [unk_token]\n",
    "    return tokens\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vocab = {'play', 'ing', 'play', 'ground', 'able', 'playground'}\n",
    "    print(greedy_subword_tokenize('playground', vocab)) \n",
    "    print(greedy_subword_tokenize('playing', vocab)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939558aa-4048-4770-bb30-6e8ead2be286",
   "metadata": {},
   "source": [
    "# 4) Byte-Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31c8f0a7-9028-47a7-843c-190edb762eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merges: [('l', 'o'), ('lo', 'w'), ('low', '</w>'), ('low', 'e'), ('lowe', 'r'), ('lower', '</w>'), ('e', 's'), ('es', 't'), ('est', '</w>'), ('n', 'e')]\n",
      "tokenized: ['lower</w>']\n"
     ]
    }
   ],
   "source": [
    "# simple_bpe.py\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[(symbols[i], symbols[i+1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    pattern = re.compile(r'(?<!\\S)'+re.escape(bigram)+r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        new_word = pattern.sub(replacement, word)\n",
    "        v_out[new_word] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "def train_bpe(corpus, num_merges=50):\n",
    "    # corpus: list of words (strings)\n",
    "    vocab = Counter()\n",
    "    for w in corpus:\n",
    "        vocab[' '.join(list(w)) + ' </w>'] += 1\n",
    "    merges = []\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "        merges.append(best)\n",
    "    return merges, vocab\n",
    "\n",
    "def apply_bpe(token, merges):\n",
    "    word = ' '.join(list(token)) + ' </w>'\n",
    "    for a,b in merges:\n",
    "        bigram = a + ' ' + b\n",
    "        word = word.replace(bigram, a+b)\n",
    "    return word.split()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    corpus = [\"low\", \"lower\", \"newest\", \"widest\", \"low\", \"low\", \"lower\"]\n",
    "    merges, final_vocab = train_bpe(corpus, num_merges=10)\n",
    "    print(\"merges:\", merges)\n",
    "    print(\"tokenized:\", apply_bpe(\"lower\", merges))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c001aa9-e277-4c6d-ac7e-2fda27de1186",
   "metadata": {},
   "source": [
    "# 5) WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf5c343d-48e3-4036-b8c4-12dad1561c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['play', '##ing', 'playground']\n"
     ]
    }
   ],
   "source": [
    "# wordpiece_greedy.py\n",
    "# WordPiece tokenization at inference time is greedy longest-match using a pre-built vocabulary.\n",
    "def wordpiece_tokenize(text, vocab, unk_token='[UNK]'):\n",
    "    tokens = []\n",
    "    for word in text.split():\n",
    "        i = 0\n",
    "        sub_tokens = []\n",
    "        while i < len(word):\n",
    "            j = len(word)\n",
    "            cur_sub = None\n",
    "            while j > i:\n",
    "                substr = (word[i:j] if i==0 else '##' + word[i:j])  # '##' denotes continuation\n",
    "                if substr in vocab:\n",
    "                    cur_sub = substr\n",
    "                    break\n",
    "                j -= 1\n",
    "            if cur_sub is None:\n",
    "                sub_tokens = [unk_token]\n",
    "                break\n",
    "            sub_tokens.append(cur_sub)\n",
    "            i = j\n",
    "        tokens.extend(sub_tokens)\n",
    "    return tokens\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vocab = {\"play\",\"##ing\",\"ground\",\"##er\",\"##est\",\"playground\",\"[UNK]\"}\n",
    "    print(wordpiece_tokenize(\"playing playground\", vocab))\n",
    "    # -> ['play','##ing','playground']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a66ed1-477c-4991-8e57-8eeaf371b5a3",
   "metadata": {},
   "source": [
    "# 6) SentencePiece "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dce046b-942c-46ce-8b70-7d6052e5fc55",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentencepiece'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# sentencepiece_example.py\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentencepiece\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspm\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 1) create a tiny training file\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sentencepiece'"
     ]
    }
   ],
   "source": [
    "# sentencepiece_example.py\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "# 1) create a tiny training file\n",
    "with open('text.txt', 'w', encoding='utf8') as f:\n",
    "    f.write(\"This is a sample sentence.\\nAnother sentence for training.\\nPlaying and playground.\\n\")\n",
    "\n",
    "# 2) train (model_type can be 'bpe', 'unigram', or 'word')\n",
    "spm.SentencePieceTrainer.Train('--input=text.txt --model_prefix=m --vocab_size=200 --model_type=bpe')\n",
    "\n",
    "# 3) load and tokenize\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('m.model')\n",
    "\n",
    "s = \"Playing playground is fun.\"\n",
    "print(sp.EncodeAsPieces(s))  # subword pieces\n",
    "print(sp.EncodeAsIds(s))     # numeric ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2cd175-0449-4b50-9e7d-2c2ca5ec208b",
   "metadata": {},
   "source": [
    "# 7) Unigram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad5af8b0-2161-4426-9c93-b99f09922260",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# train a unigram model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mspm\u001b[49m.SentencePieceTrainer.Train(\u001b[33m'\u001b[39m\u001b[33m--input=text.txt --model_prefix=unigram_m --vocab_size=100 --model_type=unigram\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# then in Python\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentencepiece\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspm\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'spm' is not defined"
     ]
    }
   ],
   "source": [
    "# train a unigram model\n",
    "spm.SentencePieceTrainer.Train('--input=text.txt --model_prefix=unigram_m --vocab_size=100 --model_type=unigram')\n",
    "\n",
    "# then in Python\n",
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('unigram_m.model')\n",
    "print(sp.EncodeAsPieces(\"Playing playground is fun.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d4f8fe-1c69-4746-9e05-af70ecca4553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
